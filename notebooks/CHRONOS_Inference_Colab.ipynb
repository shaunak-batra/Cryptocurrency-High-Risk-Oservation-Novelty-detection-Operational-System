{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# CHRONOS Model Inference\n",
                "\n",
                "This notebook runs full inference on the trained CHRONOS GNN model.\n",
                "\n",
                "**Steps:**\n",
                "1. Install dependencies\n",
                "2. Upload model checkpoint and dataset\n",
                "3. Run inference\n",
                "4. Download results"
            ],
            "metadata": {
                "id": "intro"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 1: Install Dependencies"
            ],
            "metadata": {
                "id": "step1"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "!pip install torch torch-geometric pandas numpy scikit-learn -q\n",
                "print('Dependencies installed!')"
            ],
            "metadata": {
                "id": "install"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 2: Upload Files\n",
                "\n",
                "Upload the following files:\n",
                "1. `best_model.pt` - from `checkpoints/chronos_experiment/`\n",
                "2. `elliptic_txs_features.csv` - from `data/raw/elliptic/raw/`\n",
                "3. `elliptic_txs_classes.csv` - from `data/raw/elliptic/raw/`\n",
                "4. `elliptic_txs_edgelist.csv` - from `data/raw/elliptic/raw/`"
            ],
            "metadata": {
                "id": "step2"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import files\n",
                "print('Upload best_model.pt:')\n",
                "uploaded = files.upload()"
            ],
            "metadata": {
                "id": "upload_model"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "print('Upload the 3 Elliptic CSV files:')\n",
                "uploaded = files.upload()"
            ],
            "metadata": {
                "id": "upload_data"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 3: Define Model Architecture"
            ],
            "metadata": {
                "id": "step3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch_geometric.nn import GATConv\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "class CHRONOSInference(nn.Module):\n",
                "    def __init__(self, in_features=235, hidden_dim=256, num_heads=8, dropout=0.3):\n",
                "        super().__init__()\n",
                "        self.hidden_dim = hidden_dim\n",
                "        self.num_heads = num_heads\n",
                "        \n",
                "        self.input_proj = nn.Linear(in_features, hidden_dim)\n",
                "        \n",
                "        self.temporal = nn.Sequential(\n",
                "            nn.Linear(hidden_dim, hidden_dim),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(hidden_dim, hidden_dim),\n",
                "        )\n",
                "        \n",
                "        self.gat_layers = nn.ModuleList([\n",
                "            GATConv(hidden_dim, hidden_dim // num_heads, heads=num_heads, concat=True, dropout=dropout),\n",
                "            GATConv(hidden_dim, hidden_dim // num_heads, heads=num_heads, concat=True, dropout=dropout),\n",
                "            GATConv(hidden_dim, hidden_dim, heads=num_heads, concat=True, dropout=dropout),\n",
                "        ])\n",
                "        \n",
                "        self._gat_norms = nn.ModuleList([\n",
                "            nn.BatchNorm1d(hidden_dim),\n",
                "            nn.BatchNorm1d(hidden_dim),\n",
                "            nn.BatchNorm1d(hidden_dim * num_heads),\n",
                "        ])\n",
                "        \n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.Linear(hidden_dim + hidden_dim, hidden_dim),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(hidden_dim, 2),\n",
                "        )\n",
                "    \n",
                "    def forward(self, x, edge_index):\n",
                "        h = self.input_proj(x)\n",
                "        h = F.elu(h)\n",
                "        t = self.temporal(h)\n",
                "        \n",
                "        gat_out = h\n",
                "        for gat in self.gat_layers:\n",
                "            gat_out = gat(gat_out, edge_index)\n",
                "            gat_out = F.elu(gat_out)\n",
                "            gat_out = F.dropout(gat_out, p=0.3, training=self.training)\n",
                "        \n",
                "        gat_out = gat_out.view(-1, self.num_heads, self.hidden_dim).mean(dim=1)\n",
                "        combined = torch.cat([gat_out, t], dim=-1)\n",
                "        return self.classifier(combined)\n",
                "    \n",
                "    def predict(self, x, edge_index):\n",
                "        self.eval()\n",
                "        with torch.no_grad():\n",
                "            logits = self.forward(x, edge_index)\n",
                "            probs = F.softmax(logits, dim=-1)[:, 1]\n",
                "            preds = (probs > 0.5).long()\n",
                "        return probs, preds\n",
                "\n",
                "print('Model class defined!')"
            ],
            "metadata": {
                "id": "model"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 4: Load Data and Compute Features"
            ],
            "metadata": {
                "id": "step4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import networkx as nx\n",
                "from scipy import stats\n",
                "\n",
                "print('Loading data...')\n",
                "features_df = pd.read_csv('elliptic_txs_features.csv', header=None)\n",
                "classes_df = pd.read_csv('elliptic_txs_classes.csv')\n",
                "edges_df = pd.read_csv('elliptic_txs_edgelist.csv')\n",
                "\n",
                "tx_ids = features_df[0].values\n",
                "timesteps = features_df[1].values\n",
                "X = features_df.iloc[:, 2:].values.astype(np.float32)\n",
                "\n",
                "id_to_idx = {tx_id: idx for idx, tx_id in enumerate(tx_ids)}\n",
                "\n",
                "y = np.full(len(tx_ids), -1)\n",
                "for _, row in classes_df.iterrows():\n",
                "    tx_id = row['txId']\n",
                "    if tx_id in id_to_idx:\n",
                "        idx = id_to_idx[tx_id]\n",
                "        label = str(row['class'])\n",
                "        if label == '1': y[idx] = 0\n",
                "        elif label == '2': y[idx] = 1\n",
                "\n",
                "edge_list = []\n",
                "for _, row in edges_df.iterrows():\n",
                "    src, dst = row['txId1'], row['txId2']\n",
                "    if src in id_to_idx and dst in id_to_idx:\n",
                "        edge_list.append([id_to_idx[src], id_to_idx[dst]])\n",
                "\n",
                "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
                "\n",
                "test_mask = (timesteps >= 43) & (timesteps <= 49) & (y >= 0)\n",
                "print(f'Nodes: {len(X)}, Edges: {edge_index.shape[1]}, Test samples: {test_mask.sum()}')"
            ],
            "metadata": {
                "id": "load_data"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Compute engineered features (simplified version)\n",
                "print('Computing engineered features...')\n",
                "\n",
                "G = nx.DiGraph()\n",
                "for i in range(edge_index.shape[1]):\n",
                "    G.add_edge(edge_index[0, i].item(), edge_index[1, i].item())\n",
                "\n",
                "n_nodes = len(X)\n",
                "eng_features = np.zeros((n_nodes, 70), dtype=np.float32)\n",
                "\n",
                "# Graph topology (20 features)\n",
                "in_deg = dict(G.in_degree())\n",
                "out_deg = dict(G.out_degree())\n",
                "for i in range(n_nodes):\n",
                "    eng_features[i, 0] = in_deg.get(i, 0)\n",
                "    eng_features[i, 1] = out_deg.get(i, 0)\n",
                "    eng_features[i, 2] = in_deg.get(i, 0) + out_deg.get(i, 0)\n",
                "\n",
                "print('Computing PageRank...')\n",
                "pr = nx.pagerank(G, max_iter=50)\n",
                "for i in range(n_nodes):\n",
                "    eng_features[i, 5] = pr.get(i, 0)\n",
                "\n",
                "# Temporal features (25 features) - use timestep\n",
                "for i in range(n_nodes):\n",
                "    eng_features[i, 20] = timesteps[i]\n",
                "    eng_features[i, 21] = timesteps[i] / 49.0\n",
                "\n",
                "# Normalize\n",
                "eng_features = (eng_features - eng_features.mean(axis=0)) / (eng_features.std(axis=0) + 1e-8)\n",
                "\n",
                "# Combine\n",
                "X_combined = np.concatenate([X, eng_features], axis=1)\n",
                "print(f'Combined features: {X_combined.shape[1]} dimensions')"
            ],
            "metadata": {
                "id": "features"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 5: Load Model and Run Inference"
            ],
            "metadata": {
                "id": "step5"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Load model\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f'Using device: {device}')\n",
                "\n",
                "checkpoint = torch.load('best_model.pt', map_location=device)\n",
                "model = CHRONOSInference(in_features=X_combined.shape[1])\n",
                "\n",
                "# Load weights with flexible matching\n",
                "model_state = checkpoint['model']\n",
                "model_dict = model.state_dict()\n",
                "loaded = 0\n",
                "for key in model_dict.keys():\n",
                "    ckpt_key = key\n",
                "    if '_gat_norms' in key:\n",
                "        ckpt_key = key.replace('_gat_norms', 'gat_norms')\n",
                "        parts = ckpt_key.split('.')\n",
                "        if len(parts) >= 2:\n",
                "            ckpt_key = f\"{parts[0]}.{parts[1]}.module.{'.'.join(parts[2:])}\"\n",
                "    if ckpt_key in model_state and model_dict[key].shape == model_state[ckpt_key].shape:\n",
                "        model_dict[key] = model_state[ckpt_key]\n",
                "        loaded += 1\n",
                "\n",
                "model.load_state_dict(model_dict)\n",
                "model.to(device)\n",
                "model.eval()\n",
                "print(f'Loaded {loaded}/{len(model_dict)} weights')"
            ],
            "metadata": {
                "id": "load_model"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Run inference\n",
                "print('Running inference...')\n",
                "X_tensor = torch.tensor(X_combined, dtype=torch.float32).to(device)\n",
                "edge_index = edge_index.to(device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    probs, preds = model.predict(X_tensor, edge_index)\n",
                "\n",
                "probs = probs.cpu().numpy()\n",
                "preds = preds.cpu().numpy()\n",
                "print('Inference complete!')"
            ],
            "metadata": {
                "id": "inference"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 6: Compute Metrics and Save Results"
            ],
            "metadata": {
                "id": "step6"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
                "\n",
                "# Test set metrics\n",
                "test_probs = probs[test_mask]\n",
                "test_preds = preds[test_mask]\n",
                "test_labels = y[test_mask]\n",
                "\n",
                "cm = confusion_matrix(test_labels, test_preds)\n",
                "f1 = f1_score(test_labels, test_preds)\n",
                "precision = precision_score(test_labels, test_preds)\n",
                "recall = recall_score(test_labels, test_preds)\n",
                "\n",
                "print('\\n=== TEST SET RESULTS ===')\n",
                "print(f'Confusion Matrix:')\n",
                "print(f'  TN: {cm[0,0]:5d}  FP: {cm[0,1]:5d}')\n",
                "print(f'  FN: {cm[1,0]:5d}  TP: {cm[1,1]:5d}')\n",
                "print(f'\\nF1 Score:  {f1:.4f}')\n",
                "print(f'Precision: {precision:.4f}')\n",
                "print(f'Recall:    {recall:.4f}')"
            ],
            "metadata": {
                "id": "metrics"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Save results\n",
                "cm_df = pd.DataFrame(cm, index=['Actual Licit', 'Actual Illicit'], \n",
                "                     columns=['Pred Licit', 'Pred Illicit'])\n",
                "cm_df.to_csv('confusion_matrix.csv')\n",
                "\n",
                "predictions_df = pd.DataFrame({\n",
                "    'probability': test_probs,\n",
                "    'prediction': test_preds,\n",
                "    'label': test_labels\n",
                "})\n",
                "predictions_df.to_csv('predictions.csv', index=False)\n",
                "\n",
                "metrics_df = pd.DataFrame({\n",
                "    'metric': ['f1_score', 'precision', 'recall', 'TP', 'TN', 'FP', 'FN'],\n",
                "    'value': [f1, precision, recall, cm[1,1], cm[0,0], cm[0,1], cm[1,0]]\n",
                "})\n",
                "metrics_df.to_csv('test_metrics.csv', index=False)\n",
                "\n",
                "print('Results saved!')"
            ],
            "metadata": {
                "id": "save"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Download results\n",
                "files.download('confusion_matrix.csv')\n",
                "files.download('predictions.csv')\n",
                "files.download('test_metrics.csv')"
            ],
            "metadata": {
                "id": "download"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}