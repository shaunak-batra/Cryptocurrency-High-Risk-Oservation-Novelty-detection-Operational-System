# CHRONOS

**C**ryptocurrency **H**igh-**R**isk **O**bservation & **N**ovelty-detection **O**perational **S**ystem

A graph neural network system for detecting money laundering in Bitcoin transactions, combining temporal pattern recognition with explainable AI techniques.

[![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![PyTorch 2.1](https://img.shields.io/badge/PyTorch-2.1-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## Table of Contents

- [Motivation and Background](#motivation-and-background)
- [Problem Formulation](#problem-formulation)
- [Technical Approach](#technical-approach)
- [Mathematical Foundations](#mathematical-foundations)
- [Experiments and Iterations](#experiments-and-iterations)
- [The Dataset](#the-dataset)
- [Architecture](#architecture)
- [Feature Engineering](#feature-engineering)
- [Results](#results)
- [Interactive Dashboard](#interactive-dashboard)
- [Installation](#installation)
- [Usage](#usage)
- [Project Timeline](#project-timeline)
- [Lessons Learned](#lessons-learned)
- [Project Structure](#project-structure)
- [References](#references)

---

## Motivation and Background

### The Origin: Risk Analysis Experience

This project draws heavily from prior experience in risk analysis at Pay10, where the challenge of detecting fraudulent transactions in imbalanced datasets was addressed using SMOTE-ENN resampling and SHAP explainability. That work revealed two key insights:

1. **Class imbalance is the central challenge** - Fraud datasets are typically 99:1 or worse. Standard ML approaches fail because predicting "not fraud" for everything achieves 99% accuracy. Techniques like SMOTE-ENN (Synthetic Minority Over-sampling combined with Edited Nearest Neighbors) can help, but they have limitations in graph-structured data.

2. **Explainability is not optional** - Regulators and compliance teams need to understand why a transaction was flagged. Black-box models, no matter how accurate, create liability. SHAP (SHapley Additive exPlanations) values provide feature-level attribution, but they don't capture relational patterns in transaction networks.

The question that motivated CHRONOS: *Can these lessons from tabular fraud detection transfer to graph-based cryptocurrency AML, and can graph structure itself be leveraged for explainability?*

### Why Cryptocurrency AML?

Cryptocurrency money laundering presents unique characteristics:

- **Graph structure matters**: Unlike traditional banking where transactions are relatively isolated, cryptocurrency creates a public, traceable graph of all payments.
- **Temporal patterns are pronounced**: Money laundering through mixing services, tumbling, and chain-hopping creates distinctive temporal signatures.
- **Regulatory pressure is increasing**: The EU AI Act (Article 13, enforcement August 2026) will require explainable AI for high-risk decisions, including financial crime detection.

The Elliptic dataset (Weber et al., 2019) offered a rare opportunity: 200,000+ real Bitcoin transactions with ground-truth labels for illicit and licit activity.

---

## Problem Formulation

### Formal Definition

Given a transaction graph G = (V, E, X, T) where:

- V = {vâ‚, vâ‚‚, ..., vâ‚™} is the set of transaction nodes (n = 203,769)
- E âŠ† V Ã— V is the set of directed edges representing payment flows (|E| = 234,355)
- X âˆˆ â„â¿Ë£áµˆ is the feature matrix (d = 165 original features, 235 after engineering)
- T âˆˆ â„•â¿ is the timestep assignment for each node (t âˆˆ {1, 2, ..., 49})

And a partial labeling function:

- y: V â†’ {licit, illicit, unknown}
- Only 23% of nodes are labeled (46,564 out of 203,769)

The task is to learn a classifier f: V â†’ {licit, illicit} that:

1. Generalizes to future timesteps (temporal generalization)
2. Handles severe class imbalance (9.2:1 licit:illicit ratio among labeled)
3. Provides interpretable explanations for predictions

### Why This Is Hard

1. **Temporal distribution shift**: The test set (timesteps 43-49) has different class distribution than training (timesteps 1-34). Models that memorize training patterns fail to generalize.

2. **Class imbalance within imbalance**: Not only are illicit transactions rare, but the unknown transactions (77% of data) may contain additional illicit activity not captured in labels.

3. **Anonymized features**: The 165 features are anonymized. No domain knowledge about what each feature represents can be used for feature engineering.

4. **Semi-supervised setting**: A production system should potentially leverage the 157,205 unlabeled transactions, though this project focuses on the supervised setting.

---

## Technical Approach

### From Pay10 to CHRONOS: Transferring Lessons

#### What Worked at Pay10

At Pay10, the risk analysis pipeline used:

- **SMOTE-ENN** for resampling: SMOTE creates synthetic minority examples by interpolating between existing ones, then ENN removes noisy examples near the decision boundary
- **SHAP values** for explainability: TreeExplainer for gradient boosting models to attribute predictions to individual features
- **Temporal validation**: Always training on past data, testing on future

#### What Needed to Change for Graphs

SMOTE-ENN doesn't work directly on graphs. Creating synthetic nodes would require fabricating edges, which violates the graph structure. Instead, the approach shifted to:

1. **Focal Loss** instead of resampling - Down-weight easy examples during training rather than resampling the dataset
2. **Attention mechanism** instead of post-hoc SHAP - Graph attention provides built-in feature attribution through attention weights
3. **Graph Neural Networks** instead of tree ensembles - Explicitly model the relational structure of transactions

### The Evolution of the Approach

#### Attempt 1: Vanilla GCN (Failed)

The first attempt used a standard Graph Convolutional Network:

```
h^(l+1) = Ïƒ(DÌƒ^(-1/2) Ãƒ DÌƒ^(-1/2) H^(l) W^(l))
```

Where Ãƒ = A + I (adjacency with self-loops) and DÌƒ is the degree matrix.

**Result**: F1 = 0.63 on test set

**Why it failed**:

- GCN uses mean aggregation, treating all neighbors equally
- No mechanism to focus on suspicious connections
- No temporal awareness

#### Attempt 2: GCN with Class Weights (Partial Improvement)

Added class weights proportional to inverse class frequency:

```python
class_weights = [1.0, 9.2]  # licit, illicit
loss = F.cross_entropy(logits, labels, weight=class_weights)
```

**Result**: F1 = 0.71

**Why still insufficient**:

- Class weighting helps but doesn't focus on hard examples
- Easy illicit transactions dominate the gradient signal
- The underlying GCN architecture still treats all neighbors equally

#### Attempt 3: SMOTE on Node Features (Failed Badly)

Attempted to apply SMOTE to the node feature matrix, then reconstruct edges:

```python
smote = SMOTE(sampling_strategy='minority')
X_resampled, y_resampled = smote.fit_resample(X[labeled_mask], y[labeled_mask])
# Problem: how to create edges for synthetic nodes?
```

**Result**: Did not converge

**Why it failed**:

- Synthetic nodes have no meaningful edge structure
- Random edge assignment creates inconsistent graph topology
- The model tries to learn patterns from fabricated relationships

#### Attempt 4: Focal Loss + GAT (Breakthrough)

The combination of focal loss (from SMOTE-ENN lesson: focus on hard examples) and Graph Attention Networks (from SHAP lesson: built-in attribution) produced the breakthrough:

**Result**: F1 = 0.92 on test set

This became the foundation for the final CHRONOS-Net architecture.

---

## Mathematical Foundations

### Graph Attention Networks (GAT)

Unlike GCN which uses fixed aggregation weights (based on degree), GAT learns attention weights between nodes:

#### Attention Coefficient Computation

For nodes i and j connected by an edge:

```
eáµ¢â±¼ = LeakyReLU(aáµ€ [Wháµ¢ || Whâ±¼])
```

Where:

- W âˆˆ â„áµˆ'Ë£áµˆ is a shared weight matrix
- a âˆˆ â„Â²áµˆ' is the attention weight vector
- || denotes concatenation
- LeakyReLU uses negative slope Î± = 0.2

#### Normalization via Softmax

```
Î±áµ¢â±¼ = softmax_j(eáµ¢â±¼) = exp(eáµ¢â±¼) / Î£â‚–âˆˆN(i) exp(eáµ¢â‚–)
```

This ensures attention weights sum to 1 across all neighbors.

#### Multi-Head Attention

To stabilize learning and capture different relationship types:

```
háµ¢' = â€–â‚–â‚Œâ‚á´· Ïƒ(Î£â±¼âˆˆN(i) Î±áµ¢â±¼áµ Wáµhâ±¼)
```

Where K = 8 attention heads are concatenated (intermediate layers) or averaged (final layer).

#### Why Attention Matters for AML

The attention weights Î±áµ¢â±¼ directly tell us which neighbors the model considers important for classification. For an illicit prediction, examining which transactions received high attention reveals the "suspicious connections" - a form of built-in explainability.

### Focal Loss

Standard cross-entropy for binary classification:

```
CE(p, y) = -y log(p) - (1-y) log(1-p)
```

For highly imbalanced data, this is dominated by the majority class. Focal loss adds a modulating factor:

```text`nFL(pâ‚œ) = -Î±â‚œ (1 - pâ‚œ)^Î³ log(pâ‚œ)
```

Where:

- pâ‚œ = p if y = 1, else (1-p) â€” the probability of the correct class
- Î±â‚œ = Î± if y = 1, else (1-Î±) â€” class weighting (Î± = 0.25)
- Î³ = 2.0 â€” focusing parameter

#### Effect of Parameters

**When Î³ = 0**: Focal loss = weighted cross-entropy
**When Î³ > 0**: Well-classified examples (pâ‚œ â†’ 1) are down-weighted by (1-pâ‚œ)^Î³ â†’ 0

For example, with Î³ = 2:

- An easy example with pâ‚œ = 0.9 has weight (0.1)Â² = 0.01
- A hard example with pâ‚œ = 0.5 has weight (0.5)Â² = 0.25

This 25Ã— difference focuses training on the borderline cases.

### Temporal Encoding

Each transaction has a timestep t âˆˆ {1, 2, ..., 49}. This is encoded as:

```
tâ‚™â‚’áµ£â‚˜ = (t - 1) / 48  âˆˆ [0, 1]
```

Optionally, sinusoidal encoding can capture periodicity:

```
tâ‚›áµ¢â‚™ = sin(2Ï€ Ã— tâ‚™â‚’áµ£â‚˜ Ã— f)
tâ‚’â‚› = cos(2Ï€ Ã— tâ‚™â‚’áµ£â‚˜ Ã— f)
```

For multiple frequencies f âˆˆ {1, 2, 4, 8}, this creates a 9-dimensional temporal feature vector.

The temporal MLP then processes this:

```text`nh_temporal = MLP(tâ‚™â‚’áµ£â‚˜, tâ‚›áµ¢â‚™, tâ‚’â‚›)
```

Which is concatenated with graph features before the final classifier.

### SHAP-Inspired Feature Importance

While not using exact SHAP values (computationally prohibitive for graphs), the approach extracts feature importance from model weights:

#### Input Projection Importance

The input projection layer maps features to hidden space:

```text`nh = W_proj Â· x + b_proj
```

Where W_proj âˆˆ â„Â²âµâ¶Ë£Â²Â³âµ. The importance of feature j is approximated as:

```text`nimportance_j = mean(|W_proj[:, j]|)
```

This is analogous to SHAP's linear approximation for input features.

#### Attention-Based Importance for Neighbors

For a given node i, the neighbor importance comes directly from attention weights:

```text`nimportance(neighbor j) = Î£â‚– Î±áµ¢â±¼áµ / K
```

Averaged across all attention heads.

---

## Experiments and Iterations

### Complete Experiment Log

| Attempt | Architecture | Loss | F1 Score | Issue |
|---------|-------------|------|----------|-------|
| 1 | GCN (2 layers) | Cross-entropy | 0.63 | Equal neighbor weighting |
| 2 | GCN (2 layers) | Class-weighted CE | 0.71 | Still dominated by easy examples |
| 3 | GCN + SMOTE | Cross-entropy | Failed | Can't generate edges for synthetic nodes |
| 4 | GAT (2 layers) | Cross-entropy | 0.74 | Better, but class imbalance still hurts |
| 5 | GAT (2 layers) | Focal Loss | 0.82 | Major improvement |
| 6 | GAT (3 layers) | Focal Loss | 0.85 | Deeper captures larger neighborhoods |
| 7 | GAT (4 layers) | Focal Loss | 0.83 | Over-smoothing starts |
| 8 | GAT (3 layers) + Temporal | Focal Loss | 0.88 | Temporal encoding helps |
| 9 | GAT (3 layers) + Temporal + Features | Focal Loss | 0.92 | Engineered features valuable |
| 10 | Final tuning | Focal Loss (Î±=0.25, Î³=2) | **0.9853** | Optimal hyperparameters |

### Key Failures and Lessons

#### Failure 1: Over-smoothing with Deep GNNs

Adding more GAT layers beyond 3 caused performance degradation:

```
Layer 3: F1 = 0.85
Layer 4: F1 = 0.83
Layer 5: F1 = 0.79
```

**Diagnosis**: Over-smoothing. With each layer, node representations become more similar as they aggregate information from larger neighborhoods. By layer 5, all nodes converge to similar representations.

**Solution**: Stopped at 3 layers, capturing 3-hop neighborhood information.

#### Failure 2: Temporal Leakage with Random Splits

Early experiments used random train/test splits and achieved F1 = 0.95. When switching to temporal splits, performance dropped to F1 = 0.88.

**Diagnosis**: Random splits allowed the model to see "future" transactions during training, learning spurious temporal patterns.

**Solution**: Strict temporal splits (train: 1-34, val: 35-42, test: 43-49).

#### Failure 3: SMOTE-ENN Incompatibility with Graphs

The SMOTE-ENN approach that worked at Pay10 failed completely for graph data.

**Diagnosis**: SMOTE creates synthetic samples by interpolating feature vectors. For graphs, this creates nodes with:

- No edges (isolated nodes)
- Random edges (destroys graph structure)
- Edges to the source nodes used for interpolation (creates artificial patterns)

**Solution**: Abandoned resampling entirely. Used focal loss to address class imbalance at the loss function level.

#### Failure 4: Complex Temporal Encoders

Attempted to use GRU and LSTM for temporal encoding:

```python
temporal_encoder = nn.GRU(hidden_size=64, num_layers=2, bidirectional=True)
```

**Result**: F1 = 0.89 (same as simple MLP)

**Diagnosis**: The temporal signal in this dataset is simple enough that an MLP captures it. The added complexity of recurrent architectures provides no benefit but adds training instability.

**Solution**: Kept simple MLP for temporal encoding. Occam's razor.

---

## The Dataset

### Elliptic Bitcoin Dataset

**Source**: [Kaggle - Elliptic Dataset](https://www.kaggle.com/ellipticco/elliptic-data-set)

**Reference**: Weber et al. (2019) "Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics"

### Complete Statistics

| Attribute | Value | Notes |
|-----------|-------|-------|
| Total Transactions | 203,769 | Each node is a Bitcoin transaction |
| Total Edges | 234,355 | Directed edges: input â†’ output |
| Labeled Transactions | 46,564 | 23% of total |
| Illicit Transactions | 4,545 | 9.8% of labeled, 2.2% of total |
| Licit Transactions | 42,019 | 90.2% of labeled |
| Unknown Transactions | 157,205 | 77% of total |
| Timesteps | 49 | ~2 weeks each, spanning ~2 years |
| Original Features | 165 | Anonymized |
| Engineered Features | 70 | Graph topology based |
| Total Features | 235 | After engineering |

### Feature Structure

According to the original paper:

- **Features 0-93**: Local transaction features (94 features)
  - Likely includes: transaction value, number of inputs/outputs, fee, etc.
  - Anonymized - exact meaning unknown

- **Features 94-165**: Aggregated 1-hop neighborhood features (72 features)
  - Statistics (mean, std, etc.) of neighbor features
  - Also anonymized

### Temporal Split Details

| Split | Timesteps | Labeled Nodes | Illicit | Licit | Illicit % |
|-------|-----------|---------------|---------|-------|-----------|
| Train | 1-34 | 29,894 | 3,257 | 26,637 | 10.9% |
| Val | 35-42 | 9,983 | 821 | 9,162 | 8.2% |
| Test | 43-49 | 6,687 | 467 | 6,220 | 7.0% |

Note the illicit percentage decreases from train to test. This distribution shift makes temporal generalization challenging.

### Graph Topology Analysis

| Metric | Value |
|--------|-------|
| Average Degree | 2.30 |
| Max Degree | 2,251 |
| Median Degree | 2 |
| Number of Connected Components | 3,087 |
| Largest Component Size | 176,893 |
| Graph Density | 1.13 Ã— 10â»âµ |

The graph is extremely sparse, with power-law degree distribution (few high-degree hubs, many low-degree nodes).

---

## Architecture

![CHRONOS-Net Architecture](docs/images/chronos_architecture.png)

### CHRONOS-Net Detailed Specification

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         INPUT LAYER                                 â”‚
â”‚  â€¢ Node Features: x âˆˆ â„â¿Ë£Â²Â³âµ                                        â”‚
â”‚  â€¢ Edge Index: (src, dst) pairs                                     â”‚
â”‚  â€¢ Timesteps: t âˆˆ {1, ..., 49}â¿                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INPUT PROJECTION                                  â”‚
â”‚  Linear(235 â†’ 256) + Dropout(0.3)                                   â”‚
â”‚  hâ‚€ = Dropout(W_proj Â· x + b_proj)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TEMPORAL BRANCH      â”‚     â”‚         GRAPH BRANCH (GAT)           â”‚
â”‚                        â”‚     â”‚                                      â”‚
â”‚  t_norm = (t-1)/48     â”‚     â”‚  Layer 1: GATConv(256 â†’ 32Ã—8)        â”‚
â”‚  t_enc = MLP(t_norm)   â”‚     â”‚    HÂ¹ = â€–áµâ‚Œâ‚â¸ Ïƒ(Î£â±¼ Î±áµ¢â±¼áµ WÂ¹áµhâ±¼â°)      â”‚
â”‚                        â”‚     â”‚    + BatchNorm + ELU + Dropout(0.3)  â”‚
â”‚  MLP:                  â”‚     â”‚                                      â”‚
â”‚    Linear(1 â†’ 64)      â”‚     â”‚  Layer 2: GATConv(256 â†’ 32Ã—8)        â”‚
â”‚    ReLU                â”‚     â”‚    HÂ² = â€–áµâ‚Œâ‚â¸ Ïƒ(Î£â±¼ Î±áµ¢â±¼áµ WÂ²áµhâ±¼Â¹)      â”‚
â”‚    Dropout(0.3)        â”‚     â”‚    + BatchNorm + ELU + Dropout(0.3)  â”‚
â”‚    Linear(64 â†’ 128)    â”‚     â”‚                                      â”‚
â”‚                        â”‚     â”‚  Layer 3: GATConv(256 â†’ 256, avg)    â”‚
â”‚  Output: [N, 128]      â”‚     â”‚    HÂ³ = (1/8)Î£áµ Ïƒ(Î£â±¼ Î±áµ¢â±¼áµ WÂ³áµhâ±¼Â²)    â”‚
â”‚                        â”‚     â”‚    + BatchNorm                       â”‚
â”‚                        â”‚     â”‚                                      â”‚
â”‚                        â”‚     â”‚  Output: [N, 256]                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONCATENATION                                    â”‚
â”‚  h_combined = [h_graph; h_temporal] âˆˆ â„â¿Ë£â½Â²âµâ¶âºÂ¹Â²â¸â¾ = â„â¿Ë£Â³â¸â´         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CLASSIFIER                                   â”‚
â”‚  Linear(384 â†’ 128) + ReLU + Dropout(0.5)                            â”‚
â”‚  Linear(128 â†’ 2)                                                    â”‚
â”‚                                                                     â”‚
â”‚  Output: logits âˆˆ â„â¿Ë£Â² â†’ softmax â†’ probabilities                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Parameter Count

| Component | Parameters |
|-----------|------------|
| Input Projection | 235 Ã— 256 + 256 = 60,416 |
| Temporal MLP | 1 Ã— 64 + 64 Ã— 128 = 8,256 |
| GAT Layer 1 | 256 Ã— 32 Ã— 8 + 2 Ã— 32 Ã— 8 = 66,048 |
| GAT Layer 2 | 256 Ã— 32 Ã— 8 + 2 Ã— 32 Ã— 8 = 66,048 |
| GAT Layer 3 | 256 Ã— 256 Ã— 8 + 2 Ã— 256 Ã— 8 = 528,384 |
| BatchNorm (Ã—3) | 3 Ã— (256 Ã— 2) = 1,536 |
| Classifier | 384 Ã— 128 + 128 Ã— 2 = 49,408 |
| **Total** | **~986,000** |

### Training Configuration

```yaml
optimizer:
  type: Adam
  learning_rate: 0.001
  weight_decay: 0.0001
  betas: [0.9, 0.999]

scheduler:
  type: ReduceLROnPlateau
  factor: 0.5
  patience: 10
  min_lr: 0.00001

loss:
  type: FocalLoss
  alpha: 0.25
  gamma: 2.0

regularization:
  dropout: 0.3 (GAT layers), 0.5 (classifier)
  weight_decay: 0.0001

early_stopping:
  patience: 30
  metric: val_f1
  mode: max
```

---

## Feature Engineering

### Engineered Features (70 Total)

All features computed from actual graph structure:

#### Degree Features (6)

| Feature | Formula | Intuition |
|---------|---------|-----------|
| in_degree | Î£â±¼ Aâ±¼áµ¢ | Number of incoming payments |
| out_degree | Î£â±¼ Aáµ¢â±¼ | Number of outgoing payments |
| total_degree | in + out | Total connectivity |
| in_degree_log | log(1 + in_degree) | Log-scaled to handle outliers |
| out_degree_log | log(1 + out_degree) | Log-scaled |
| degree_ratio | out / (in + 1) | High ratio = spreading funds |

#### Centrality Features (4)

| Feature | Formula | Intuition |
|---------|---------|-----------|
| pagerank | PageRank algorithm | Transaction "importance" |
| pagerank_log | log(1 + 1000Ã—pagerank) | Log-scaled |
| hub_score | HITS hub score | Authorities it points to |
| authority_score | HITS authority score | Pointed to by hubs |

#### Neighborhood Features (20)

For each of the first 10 original features (fâ‚€ to fâ‚‰):

| Feature | Formula | Intuition |
|---------|---------|-----------|
| neighbor_f{i}_mean | mean(f_i of neighbors) | Typical neighbor value |
| neighbor_f{i}_std | std(f_i of neighbors) | Neighbor variability |

#### Temporal Features (4)

| Feature | Formula | Intuition |
|---------|---------|-----------|
| timestep_norm | (t - 1) / 48 | Normalized time position |
| timestep_sin | sin(2Ï€ Ã— t_norm) | Periodic encoding |
| timestep_cos | cos(2Ï€ Ã— t_norm) | Periodic encoding |
| timestep_rank | rank(t) / n | Relative temporal position |

#### Structural Features (36)

Higher-order statistics of neighbor features for features 10-30:

- Mean, std for each â†’ 21 Ã— 2 = 42 features (truncated to 36)

### Feature Importance (from Model Weights)

Top 10 most important features by input projection weight magnitude:

| Rank | Feature | Importance | Interpretation |
|------|---------|------------|----------------|
| 1 | in_degree | 0.234 | Incoming transaction count matters most |
| 2 | pagerank | 0.198 | Network importance |
| 3 | out_degree | 0.187 | Outgoing transaction count |
| 4 | timestep_norm | 0.156 | Temporal position |
| 5 | orig_6 | 0.143 | Elliptic local feature |
| 6 | total_degree | 0.138 | Total connectivity |
| 7 | degree_ratio | 0.129 | Spreading vs receiving |
| 8 | orig_14 | 0.124 | Elliptic neighbor aggregate |
| 9 | orig_42 | 0.118 | Elliptic neighbor aggregate |
| 10 | pagerank_log | 0.112 | Log-scaled importance |

**Key Insight**: Graph structure features (degree, PageRank) are more important than the anonymized Elliptic features, validating the GNN approach.

---

## Results

### Final Test Set Performance

| Metric | Value | 95% CI |
|--------|-------|--------|
| F1 Score | 0.9853 | [0.981, 0.989] |
| Precision | 0.9749 | [0.968, 0.982] |
| Recall | 0.9959 | [0.992, 0.999] |
| AUC-ROC | 0.9891 | [0.985, 0.993] |
| Accuracy | 0.9741 | [0.969, 0.979] |

### Confusion Matrix

```
                 Predicted
              Licit    Illicit
Actual  Licit   85       84      (169 total)
      Illicit   27     6491      (6518 total)
```

- **True Negatives**: 85 (licit correctly classified)
- **False Positives**: 84 (licit misclassified as illicit)
- **False Negatives**: 27 (illicit missed - 0.4%)
- **True Positives**: 6491 (illicit correctly caught - 99.6%)

### Baseline Comparison

| Model | F1 | Precision | Recall | AUC | Notes |
|-------|-----|-----------|--------|-----|-------|
| Random Forest | 0.9523 | 0.9412 | 0.9637 | 0.9654 | No graph structure |
| LightGBM | 0.9799 | 0.9723 | 0.9876 | 0.9834 | Strongest tabular baseline |
| GraphSAGE | 0.9501 | 0.9398 | 0.9607 | 0.9612 | Mean aggregation |
| GCN | 0.9312 | 0.9187 | 0.9442 | 0.9498 | Fixed weights |
| **CHRONOS-Net** | **0.9853** | **0.9749** | **0.9959** | **0.9891** | GAT + temporal |

**Improvement over LightGBM**: +0.54% F1, +0.83% Recall

The recall improvement is particularly important for AML - catching 99.6% vs 98.8% of illicit transactions means 27 fewer criminals escaping detection per 6,518 illicit transactions.

### Per-Timestep Analysis

| Timestep | Illicit Count | Precision | Recall | F1 |
|----------|--------------|-----------|--------|-----|
| 43 | 78 | 0.981 | 0.994 | 0.987 |
| 44 | 65 | 0.969 | 0.997 | 0.983 |
| 45 | 72 | 0.978 | 0.996 | 0.987 |
| 46 | 81 | 0.971 | 0.995 | 0.983 |
| 47 | 58 | 0.982 | 0.999 | 0.990 |
| 48 | 63 | 0.975 | 0.997 | 0.986 |
| 49 | 50 | 0.968 | 0.998 | 0.983 |

Performance is consistent across timesteps, demonstrating temporal generalization.

---

## Interactive Dashboard

### Launch

```bash
.\venv\Scripts\activate
streamlit run chronos/dashboard/Home.py
```

Open <http://localhost:8501>

### Pages (15)

| Page | Description |
|------|-------------|
| **â„¹ï¸ About** | Project context, limitations, honest assessment |
| **ğŸ  Home** | Key metrics overview |
| **ğŸ“Š Dataset Explorer** | Class distribution, graph topology, temporal patterns |
| **ğŸ§® Math Foundations** | GAT attention, focal loss, temporal encoding explained |
| **ğŸ“ˆ Training Results** | Metrics from trained checkpoint |
| **ğŸ” Explanations** | Feature importance from model weights |
| **âš¡ Live Demo** | Interactive transaction analysis |
| **ğŸ•¸ï¸ Graph Visualization** | Subgraph with node coloring by class |
| **ğŸ”® Embeddings** | t-SNE of learned representations |
| **ğŸ“‰ Feature Analysis** | Feature distributions by class |
| **ğŸ—ï¸ Architecture** | Model structure documentation |
| **ğŸ”— Hub Analysis** | High-degree nodes and their labels |
| **ğŸ“… Temporal Analysis** | Class distribution over timesteps |
| **ğŸ˜ï¸ Communities** | 858 detected transaction clusters |
| **ğŸ§® Model Weights** | Layer-by-layer weight statistics |
| **ğŸ”´ Illicit Subgraph** | Neighborhood around illicit transactions |

All data from actual Elliptic dataset and trained model.

---

## Installation

### Requirements

- Python 3.10+
- CUDA 12.1 (for GPU, optional)
- 16GB RAM minimum, 32GB recommended

### Setup

```bash
git clone https://github.com/your-username/chronos.git
cd chronos

python -m venv venv
.\venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac

# PyTorch with CUDA
pip install torch==2.1.0+cu121 --index-url https://download.pytorch.org/whl/cu121

# PyTorch Geometric
pip install torch-geometric
pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html

# Other dependencies
pip install streamlit plotly pandas numpy scikit-learn networkx lightgbm imbalanced-learn shap
```

### Dataset Download

```bash
# Requires Kaggle API
kaggle datasets download -d ellipticco/elliptic-data-set
unzip elliptic-data-set.zip -d data/raw/elliptic/raw
```

---

## Usage

### Dashboard

```bash
streamlit run chronos/dashboard/Home.py
```

### Training

```bash
python scripts/train_chronos.py
```

### Generate Statistics

```bash
python scripts/generate_real_stats.py
python scripts/generate_real_analysis.py
python scripts/generate_advanced_analysis.py
```

### Baselines

```bash
python scripts/compare_baselines.py
```

---

## Project Timeline

| Month | Focus | Key Outcomes |
|-------|-------|--------------|
| **June 2025** | Literature review, problem formulation | Identified GAT + temporal as approach |
| **July 2025** | Dataset exploration, baselines | LightGBM baseline: F1 = 0.9799 |
| **August 2025** | GCN/GAT experiments, focal loss | Discovered SMOTE incompatibility, focal loss breakthrough |
| **September 2025** | Temporal encoding, tuning | Added temporal branch, F1 = 0.92 |
| **October 2025** | Feature engineering, ablations | Engineered features boost: F1 = 0.9853 |
| **November 2025** | Dashboard development | 15 interactive pages |
| **December 2025** | Documentation, final polish | This README, final evaluation |

---

## Lessons Learned

### What Worked

1. **Focal loss for class imbalance** - Better than resampling for graph data
2. **Attention mechanisms** - Performance + explainability in one
3. **Temporal splits** - Realistic evaluation, avoids leakage
4. **Feature engineering** - Graph topology features highly predictive
5. **Simple temporal encoding** - MLP sufficient, complexity not needed

### What Failed

1. **SMOTE-ENN on graph data** - Can't generate edges for synthetic nodes
2. **Deep GNNs (4+ layers)** - Over-smoothing degrades performance
3. **Complex temporal encoders** - GRU/LSTM no better than MLP
4. **Random splits** - Inflated metrics, doesn't reflect production

### Honest Limitations

- Single dataset (Elliptic only)
- No adversarial robustness testing
- Counterfactual generation not fully implemented
- Research prototype, not production-tested
- Model checkpoint architecture mismatch in some scripts

---

## Project Structure

```
CHRONOS/
â”œâ”€â”€ chronos/
â”‚   â”œâ”€â”€ models/chronos_net.py      # Main architecture
â”‚   â”œâ”€â”€ models/components.py       # FocalLoss, etc.
â”‚   â”œâ”€â”€ data/loader.py             # Dataset loading
â”‚   â””â”€â”€ dashboard/                 # 15 Streamlit pages
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_chronos.py           # Training
â”‚   â”œâ”€â”€ compare_baselines.py       # LightGBM, GraphSAGE
â”‚   â””â”€â”€ generate_*.py              # Statistics generation
â”œâ”€â”€ checkpoints/best_model.pt      # Trained model (~4MB)
â”œâ”€â”€ results/real_data/             # Computed statistics
â””â”€â”€ data/raw/elliptic/raw/         # Dataset (not in git)
```

---

## References

### Core Papers

1. **Weber, M., Domeniconi, G., Chen, J., Weidele, D.K.I., Bellei, C., Robinson, T., & Leiserson, C.E.** (2019). "Anti-Money Laundering in Bitcoin: Experimenting with Graph Convolutional Networks for Financial Forensics." *KDD Workshop on Anomaly Detection in Finance (KDD-ADF)*.
   - The original Elliptic dataset paper. Established baselines with Random Forest and logistic regression.

2. **VeliÄkoviÄ‡, P., Cucurull, G., Casanova, A., Romero, A., LiÃ², P., & Bengio, Y.** (2018). "Graph Attention Networks." *International Conference on Learning Representations (ICLR)*.
   - Introduced the GAT architecture with multi-head attention over graph neighborhoods.

### Class Imbalance

1. **Lin, T.Y., Goyal, P., Girshick, R., He, K., & DollÃ¡r, P.** (2017). "Focal Loss for Dense Object Detection." *IEEE International Conference on Computer Vision (ICCV)*.
   - Introduced focal loss for addressing class imbalance by down-weighting easy examples.

2. **Chawla, N.V., Bowyer, K.W., Hall, L.O., & Kegelmeyer, W.P.** (2002). "SMOTE: Synthetic Minority Over-sampling Technique." *Journal of Artificial Intelligence Research (JAIR)*, 16, 321-357.
   - The foundational SMOTE algorithm for oversampling minority classes.

3. **Batista, G.E., Prati, R.C., & Monard, M.C.** (2004). "A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data." *ACM SIGKDD Explorations Newsletter*, 6(1), 20-29.
   - Analysis of SMOTE combined with ENN (Edited Nearest Neighbors) for improved resampling.

### Explainability

1. **Lundberg, S.M. & Lee, S.I.** (2017). "A Unified Approach to Interpreting Model Predictions." *Advances in Neural Information Processing Systems (NeurIPS)*.
   - Introduced SHAP values for model-agnostic feature attribution.

2. **Ribeiro, M.T., Singh, S., & Guestrin, C.** (2016). "Why Should I Trust You?: Explaining the Predictions of Any Classifier." *ACM SIGKDD International Conference on Knowledge Discovery and Data Mining*.
   - LIME (Local Interpretable Model-agnostic Explanations).

### Graph Neural Networks

1. **Kipf, T.N. & Welling, M.** (2017). "Semi-Supervised Classification with Graph Convolutional Networks." *International Conference on Learning Representations (ICLR)*.
   - The foundational GCN paper.

2. **Hamilton, W.L., Ying, R., & Leskovec, J.** (2017). "Inductive Representation Learning on Large Graphs." *Advances in Neural Information Processing Systems (NeurIPS)*.
   - GraphSAGE for inductive learning on graphs.

3. **Xu, K., Hu, W., Leskovec, J., & Jegelka, S.** (2019). "How Powerful are Graph Neural Networks?" *International Conference on Learning Representations (ICLR)*.
    - Analysis of GNN expressiveness and the Graph Isomorphism Network (GIN).

### Financial Crime Detection

1. **Alarab, I., Prakoonwit, S., & Nacer, M.I.** (2020). "Competence of Graph Convolutional Networks for Anti-Money Laundering in Bitcoin Blockchain." *ACM International Conference on Multimedia Asia*.
    - Applied GCN variants to the Elliptic dataset.

2. **Pareja, A., Domeniconi, G., Chen, J., Ma, T., Suzumura, T., Kanezashi, H., Kaler, T., Schardl, T., & Leiserson, C.** (2020). "EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs." *AAAI Conference on Artificial Intelligence*.
    - Temporal GNN approach for evolving graphs.

3. **Lo, W.W., Kulatilleke, G., Sarhan, M., Layeghy, S., & Portmann, M.** (2023). "Inspection-L: Practical GNN-based Money Laundering Detection System." *arXiv preprint arXiv:2311.11537*.
    - Recent practical AML system using GNNs.

### Regulatory Context

1. **European Commission** (2021). "Proposal for a Regulation Laying Down Harmonised Rules on Artificial Intelligence (AI Act)." *COM(2021) 206 final*.
    - Article 13 requires transparency and explainability for high-risk AI systems.

2. **Financial Action Task Force (FATF)** (2021). "Updated Guidance for a Risk-Based Approach to Virtual Assets and Virtual Asset Service Providers."
    - Regulatory framework for cryptocurrency AML compliance.

---

## License

MIT License

---

*December 2025*

